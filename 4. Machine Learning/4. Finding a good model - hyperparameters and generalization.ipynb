{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding a good model - hyperparameters and generalization\n",
    "Neural networks are only a tool, not a ready-to-go solution for every given problem. To get a model that can reliably predict good values, we need to investigate which hyperparameteres work best - and what our results actually tell us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shit in, shit out - data scaling\n",
    "We were not very nice during the preparation of the data. Neural networks work a lot better if they are presented *normalized data*. The best performance in many cases can be achieved using data that has a mean value of $\\mu = 0$ and a standard deviation of $\\sigma=1$. What about our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you run this in Colab, you need to download the examples: uncomment the following line\n",
    "# ! git clone https://github.com/flome/e4_bsc_python\n",
    "# % cd e4_bsc_python\n",
    "# ! git checkout machine_learning\n",
    "# % cd 4.\\ Machine\\ Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import the data into a pandas DataFrame\n",
    "data = pd.read_csv('circle_data.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean values are approximately zero, but the data is spread out further than recommended. Scaling data to the mentioned requirements can be done with the equation\n",
    "<center>\n",
    "    $x^\\prime = \\frac{x-\\hat{x}}{\\sigma}$\n",
    "</center>\n",
    "where $\\hat{x}$ is the mean value of the feature and $\\sigma$ its standard deviation. We don't need to do this by hand, there are scalers available from e. g. the scikit learn Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = data[['x','y']].values\n",
    "Y = data['class'].values\n",
    "\n",
    "# fit computes the mean value and the standard deviation per feature\n",
    "# transform applies it to the design matrix\n",
    "# fit_transform does both in one step\n",
    "# get the design matrix and the target vector\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative scaling method that is used a lot is the MinMaxScaler, which scales the data to a given range like (-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "You tried your best (hopefully) to get some improvements out of your model by varying the *hyperparameters* of your model. To avoid doing monkey-work and to get results you can use for your thesis, a systematic approach to test parameters is important. If you need to test a lot of parameter combinations, it can be extremely useful to collect the results and save them, so that you can analyse which parameters had which impact on your model.\n",
    "\n",
    "We start by defining a function that returns a Keras model using parameters we pass to it. We will see, that dictionaries are very useful for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameter_combination(parameters, X, Y):\n",
    "    # create a Sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(2,)))\n",
    "    \n",
    "    # let's keep the number of nodes in the hidden layer and the activation variable\n",
    "    model.add(Dense(parameters['hidden_nodes'], activation=parameters['hidden_activation']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "   \n",
    "    # we can also vary the optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer=parameters['optimizer'], metrics=['accuracy'])\n",
    "    \n",
    "    # we want to fit the model and return the trained model and the loss history for later inspection\n",
    "    # verbose=0 makes the progress outputs go away\n",
    "    loss_history = model.fit(X, Y, epochs=500, verbose=0)\n",
    "    return model, loss_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out: We want to train three networks to start with. One with 10 hidden nodes, one with 40 and one with 80. For the hidden layer, we will use a *tanh* function, which is very similar to the sigmoid function but has an output in the range (-1,1). It tends to converge faster if used in hidden layers than the sigmoid function.\n",
    "Apart from that, we will keep the activation and the optimizer constant for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameter combinations:\n",
    "parameters=[\n",
    "    {'hidden_nodes': 10, 'hidden_activation': 'tanh', 'optimizer': 'adam'},\n",
    "    {'hidden_nodes': 40, 'hidden_activation': 'tanh', 'optimizer': 'adam'},\n",
    "    {'hidden_nodes': 80, 'hidden_activation': 'tanh', 'optimizer': 'adam'}\n",
    "]\n",
    "# create a list that will be filled with the results:\n",
    "losses = []\n",
    "models = []\n",
    "# loop over the parameters\n",
    "# this will take a while!\n",
    "for parameter in parameters:\n",
    "    print(\"Testing parameter configuration: {}\".format(parameter))\n",
    "    model, loss = test_parameter_combination(parameter, X_scaled, Y)\n",
    "    models.append(model)\n",
    "    \n",
    "    # we will be happy about this weird looking bit just in a second\n",
    "    losses.append({**parameter, 'loss_history': loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of the results. Let's see what is going on there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of list: \", len(losses))\n",
    "print(\"Content of each loss history:\", losses[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we put this weird `{**parameter, 'loss': loss}` bit in there? It makes it really easy to store results in a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(losses)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can introduce new columns to show the loss and the accuracy after the last training epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['final_loss'] = res['loss_history'].apply(lambda x: x['loss'][-1])\n",
    "res['final_acuracy'] = res['loss_history'].apply(lambda x: x['accuracy'][-1])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the loss curves to see, which training was more successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for run, loss in enumerate(losses):\n",
    "    plt.plot(loss['loss_history']['loss'], '.', label='run {}'.format(run))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('b. c. e loss')\n",
    "plt.legend(loc=2, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model performs best? What about the accuracy of the models? Try to investigate them by looking at the `loss['loss_history']['accuracy']` curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### excursus: smarter parameter combinations with itertools\n",
    "The way we defined the parameter combination was a possible one but certainly not the most efficient one. Let's define a *parameter grid* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid={\n",
    "    'hidden_nodes': [10, 40, 80],\n",
    "    'hidden_activation': ['tanh'],\n",
    "    'optimizer': ['adam']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get the needed parameter combinations that we want to pass for our training? The *itertools* package has got us covered! This may look painful for a moment, but then it is really enjoyable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def get_param_combos(p_grid):\n",
    "    combis = product(*[v for v in p_grid.values()])\n",
    "    return [{key: value for key, value in zip(p_grid.keys(), combo)} for combo in combis] \n",
    "\n",
    "combos = get_param_combos(parameter_grid)\n",
    "print(combos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q. e. d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and overtraining\n",
    "You suddenly stumble upon a new chunk of data, that also belongs to your data set. That's interesting. Let's have a look at our model's performance on this *unseen* test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('test_data.csv', index_col=0)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to do the preprocessing bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = new_data[['x', 'y']].values\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = new_data['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check our model performances using the `evaluate` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(\"Performance on new data: \", model.evaluate(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_for_mlxtend = Y_test.flatten().astype(int) # plot_decision_regions needs a 1D int array\n",
    "for i, model in enumerate(models):\n",
    "    plt.figure()\n",
    "    plot_decision_regions(X_test, Y_for_mlxtend, clf=model)\n",
    "    plt.title('run {}'.format(i))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(loc=2, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are lucky, our model *generalized* very well the underlying distribution! This time everything worked out nicely, but this is not necessarily always the case. If the model draws the decision boundary too tightly around the data points, it does not approximate the underlying function but only *memorizes* the data points. This is called *over-fitting* and it is a massive problem for machine learning both in classification and regression if not addressed accordingly.\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://miro.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png\" width=600px style='width: 600px\n",
    "      '> \n",
    "        <img src=\"https://miro.medium.com/max/1400/1*JZbxrdzabrT33Yl-LrmShw.png\" width=600px style='width:600px\n",
    "      '>\n",
    "        (images: https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n",
    "    </center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data\n",
    "To reduce the probability of over-fitting the data, it is very important to *anticipate* the arrival of new, unseen data. We do this by keeping some data out of the weight-update process and only use it for *validation* of the training progress. Decisions on the model architecture like hyperparameters can than be trained on the training part of the data set before the *generalization performance* is estimated by evaluating the validation set. Keras can do this on the fly during the training process by specifying a `validation_split`, how much of the training data should be left out for the weight updates. You should leave out yet another bit of your data that is not considered at all during the optimization phase of your model training. This bit our your data will be called *test data*. This is important to prevent *over-training* with respect to the validation data and thereby improving the overall *generalization*\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://cdn-media-1.freecodecamp.org/images/augTyKVuV5uvIJKNnqUf3oR1K5n7E8DaqirO\" height=400px style='height:400px'>\n",
    "        (image: https://www.freecodecamp.org/news/how-to-get-a-grip-on-cross-validations-bb0ba779e21c/)\n",
    "    </center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(2,)))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# use 15% of the data for training validation\n",
    "loss_history = model.fit(X_scaled, Y, epochs=500, validation_split=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inspecting the training of a machine learning model, it is always very important to watch both the *training* and *validation* loss. In our very simple model, both converge with more epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history.history['loss'], '.', label='training data')\n",
    "plt.plot(loss_history.history['val_loss'], '.', label='validation data')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('b. c. e loss')\n",
    "plt.legend(loc=2, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for a generalizing model is always, to have a very similar if not even same score for training, validation and test data. On real data sets this is normally not achieved but with carefully monitoring the loss curves we can test and improve the generalization performance\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://i.stack.imgur.com/rpqa6.jpg\" height=400px style='height:400px'>\n",
    "        (image: https://stats.stackexchange.com/questions/292283/general-question-regarding-over-fitting-vs-complexity-of-models)\n",
    "    </center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting a test set\n",
    "You have seen how to automatically assign a part of the data as validation set. How about test data? Of course you could split the data set by hand in two parts, but we prefer using (like always) to use already ready-to-use solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to *first* split the data, and then create the scalers, otherwise the effect of scaling the data is not properly included in the testing process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, you can also create a dedicated validation set of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
