{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tool to fit them all - basics of neural networks\n",
    "In the last tutorial we learned what regression and classification problems are. Basically we want to \"predict\" the most likely value for a given input. This can be a continuous variable in the case of regression - like the price of a house based on some parameters like is size and location. It can also be a class like the kind of animal we see on a picture. \n",
    "In the first tutorial everything was linear. We used only a linear function to do regression and we used a straight line to draw a decision boundary between two groups with class 1 and class 2.\n",
    "This approach is for obvious reasons too simple for nearly all practical use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code so that you can enjoy the examples!\n",
    "# if you run this in Colab, you need to download the examples: uncomment the following line\n",
    "# ! git clone https://github.com/flome/e4_bsc_python\n",
    "# % cd e4_bsc_python\n",
    "# ! git checkout machine_learning\n",
    "# % cd 4.\\ Machine\\ Learning\n",
    "from interactive_examples import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dcf91245a944c3a1fede9c2b83274e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output())), VBox(children=(FloatSlider(value=0.0, continuous_update=Fa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code\n",
    "circle_and_the_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Of neurons and networks\n",
    "Neural networks work very similar to what we have done so far but they use more combination of input parameters. Let's put some new labels on what we are doint at the moment. We will use the following illustration which shows a *neuron*:\n",
    "\n",
    "<img src='https://github.com/flome/e4_bsc_python/blob/machine_learning/4.%20Machine%20Learning/imgs/simple_neuron.png?raw=true' height=150 style='height: 150px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of $m$ and $b$ are called *weights* in the language of neural networks. $x$ is a *feature*, an input variable, $b$ is the *bias*. So far, we only used $x$ as explicit input variable, $y$ was completely dependent on $x$ using $m$ and $b$. Upon closer inspection, we find that this is not really, what our classification is doing, is it? This *neuron* does not produce a class probability. Where is the *sigmoid function* and what is actually its input?\n",
    "A straight line $y = m\\cdot x + b$ can be rewritten as \n",
    "<p>\n",
    "    <center>\n",
    "        $0 = m\\cdot x + (-1)\\cdot y + b\\cdot(1)$\n",
    "    </center>\n",
    "</p>\n",
    "The equation holds true for all points on the line. What happens, if y is larger than $m\\cdot x + b)$? The right side does not compute to $0$ anymore but results in something smaller. In turn, if $y$ is smaller, something larger than $0$ is its result. If we want to assign a value to every point we see, we need to write an equation like the following:\n",
    "<p>\n",
    "    <center>\n",
    "        $f(x, y) = w_1\\cdot x + w_2\\cdot y + w_0\\cdot(1)$\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "Now we are a lot closer to what we actually see in the output of the first example. But wait, where is the *sigmoid* function we introduced to get our class labels is not in our neuron model? The sigmoid function is what is called *activation* function in the language of neural networks. Activation functions are applied to the outputs of neurons. We now have a final output equation of our neuron:\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $\\sigma(x, y) = \\mathrm{sigmoid}\\left(w_1\\cdot x + w_2\\cdot y + w_0\\cdot(1)\\right)$\n",
    "    </center>\n",
    "</p>\n",
    "<p>\n",
    "    <img src='https://github.com/flome/e4_bsc_python/blob/machine_learning/4.%20Machine%20Learning/imgs/simple_neuron_complete.png?raw=true' height=250 style='height: 250px'>\n",
    "</p>\n",
    "\n",
    "The *nodes* on the left side are our *input features* and the *bias*, the *weights* are the factors they are multiplied with in a sum. The sum is the *activation* of the *output node*. The final output is computed by applying the *activation* function to the *activation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where the magic happens - hidden layers\n",
    "This seems to be just a fancy and complicated way to write a linear equation. But the magic of neural networks starts as soon as we don't use the neuron output as our final result, but an intermediate step. We can compute more than one output from our two *features* and our *bias*, we just need to choose different weights. We end up with two outputs. Those can then be combined by yet two new weights and a bias to the final output:\n",
    "\n",
    "<p>\n",
    "    <img src='https://github.com/flome/e4_bsc_python/blob/machine_learning/4.%20Machine%20Learning/imgs/simple_neuron_complex.png?raw=true' height=300 style='height: 300px'>\n",
    "</p>\n",
    "\n",
    "The intermediate neurons are *hidden* and are therefore called *hidden neurons*. Neurons can be grouped in *layers*. We have the *input layer*, the *hidden layer* and the *output layer*. Note, that nothing big has happened. The final result is still a function of the input features and the bias values. But check out what big difference it makes for your classification powers. Due to coding simplicity, no *decision border* is shown. All predicted values are rounded to 1 or 0 to make the results more visible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178c3e7e4ebe496b90ea9450c4b51d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), VBox(children=(FloatSlider(value=-3.46, continuous_update=False, description='$w_{0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "circle_with_hidden_neurons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time adjusting some parameters. Can you see, how the weights of the input features and the biases change the network output? You can clearly see how the *non-linearity* of the sigmoid functions impact the output of the *network*. You see, how simple linear combinations, together with non-linear activations, can already approximate quite arbitrary decision functions. \n",
    "\n",
    "After a while, you will surely figure out, that this network is to simple to get the circle done. The more hidden neurons and hidden layers we add to our network, the more complex functions it can approximate! Deeper networks can therefore approximate pretty much any shape!\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://miro.medium.com/max/2460/1*KHs1Chs6TCJDTIIQVyIJxg.png\" height=300 style='height: 300px'>\n",
    "    </center>\n",
    "    (image: https://towardsdatascience.com/a-laymans-guide-to-deep-neural-networks-ddcea24847fb)\n",
    "</p>\n",
    "\n",
    "Apart from being way more capable than before, this also became awefully difficult to adjust! We don't want to keep you on the sliders and I don't want to write self-made optimizers for neural networks. Therefore we will move to *Keras*, a Python framework to create, train and use neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
