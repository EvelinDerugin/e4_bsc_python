{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and classification - supervised learning\n",
    "Machine learning is a very broad field and pretty much covers every application in which a bit of software is not explicetly programmed to perform a special task but *learns* to do it by *training* it with example data. This tutorial will only cover one aspect of machine learning: *supervised learning*. \n",
    "\n",
    "Supervised learning is always about predicting something based on some underlying data:\n",
    "    - product interests based on Google searches\n",
    "    - stock prices based on the market development of the last week\n",
    "    - the kind of animal we see in an image based on samples we have stored in a data base\n",
    "    \n",
    "Machine learning is called *supervised*, if the value that is to be predicted is available for the data that is used for training. If an algorithm is trained to separate images of cats and dogs, the images used for training this are labelled by someone before the training. In the beginning, a machine learning algorithm won't be able to keep them apart, but after it has been *trained* on a lot of data, it can be able to perform this task.\n",
    "\n",
    "There are many types of algorithms which can be used for supervised learning. Some of them are shown below. In this tutorial, we will focus on one special type of machine learning algorithms: *neural networks*\n",
    "<img src=\"https://miro.medium.com/max/477/1*KFQI59Yv7m1f3fwG68KSEA.jpeg\" height=500 style='height: 500px'> (image: https://medium.com/technology-nineleaps/popular-machine-learning-algorithms-a574e3835ebb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code so that you can enjoy the examples!\n",
    "# if you run this in Colab, you need to download the examples: uncomment the following line\n",
    "# !wget https://github.com/flome/e4_bsc_python/blob/machine_learning/4.%20Machine%20Learning/interactive_examples.py\n",
    "from interactive_examples import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can understand the basics of machine learning and neural networks, we need to explore which problems we want to solve with it and how a computer sees those problems. We will discuss two basic problem: regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Regression deals with the prediction of continuous variables based on input data. The simplest form of regression is a function fit with one variable as it is done in the lab exercises a lot:\n",
    "<img src=\"https://github.com/flome/e4_bsc_python/blob/machine_learning/4.%20Machine%20Learning/imgs/linear_fit.png?raw=true\" height=400 style='height: 400px'>\n",
    "If we now do a measurement at a new point *x'*, the regression can be used to \"predict\" the most likely value for *y = f(x')*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "The regression method used above is called *linear regression* for obvious reasons. The regression prediction is determined by a *slope m* and an *intercept b*. In the example below, you can experiment a bit how to match the data well: Adjust the slope and intercept, so that the *manual linear fit* matches the data well. The plot updates as soon as you stop moving the sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6721201ae145998002119617ed5a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), VBox(children=(FloatSlider(value=0.0, continuous_update=False, description='slope m:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "linear_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions\n",
    "Turns out, this is really annoying because you don't even know, what \"matching the data well\" is even suppose to mean! The computer cannot know this either. We need to quantify what \"matching the data\" means. This is done with *loss* or *cost* functions. \n",
    "\n",
    "A loss function returns a value based on \"how well the fit matches the data\". Usually, a *lower cost* corresponds with a better agreement with the data. For regression, the *least-squares-fit* is the most common way to quantify this. The loss is computed as the average squared difference of the data values $y_i$ from the fit line $f(x_i)$:\n",
    "<p>\n",
    "<center>\n",
    "$\\mathrm{m. s. e.} = \\frac{1}{N} \\sum_{i = 0}^{N} \\left( y_i - f(x_i) \\right)^2$\n",
    "</center>\n",
    "</p>\n",
    "Try your luck again, this time you get to know \"how well you match the data\". How low can you get the loss?\n",
    "The plots are updated when you stop moving the sliders!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1481e2fde79b4565aed39ebfd7a75885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output())), VBox(children=(FloatSlider(value=0.0, continuous_update=Fa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "linear_example_with_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monkey work - automate curve fitting using a minimizer\n",
    "This is the way that a computer sees a fitting problem. You have probably followed a certain strategy during the loss *minimization*. While it can be possible in 2 dimensions ( *m* and *b* ) to just keep trying what change lowers the loss, this becomes a very difficult problem in more dimensions. \n",
    "\n",
    "You don't (of course you don't) need to do this by hand for more complex *parameter optimizations*. Computer programs which are designed to be specifically good at this are called *minimizier* or *optimizer* and the strategies they implement are quite different.\n",
    "Most minimizers implement the concept of *gradient descent*. If the loss function is differentiable with respect to the parameters of our function, we can compute the derivative of the loss. This means, that we can estimate, how much our loss will change if we move a parameter to higher or lower values! This makes getting to the best result a lot easier! \n",
    "<p>\n",
    "<img src=\"https://blog.paperspace.com/content/images/2018/05/fastlr.png\" height=250, style='height: 250px'>\n",
    "</p>\n",
    "(image: https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)\n",
    "\n",
    "How much do we have to change the parameters as soon as we have computed the best change of parameters? That depends! The *rate* of parameter change relative to the loss gradients is often called *learning rate* and is one of the most important parameters in optimizations. \n",
    "<p>\n",
    "<img src=\"https://miro.medium.com/max/1200/0*K0ltbXIgtNLEXsXN.png\" height=250, style='height: 250px'>\n",
    "</p>\n",
    "(image: https://medium.com/octavian-ai/how-to-use-the-learning-rate-finder-in-tensorflow-126210de9489)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the new *Descend!* button in the example below. It changes the parameters $m$ and $b$ of the linear function according to a simple gradient descent rule\n",
    "\n",
    "<center>\n",
    "    $ \\vec{\\lambda}_{i+1} = \\vec{\\lambda}_i - \\alpha \\cdot \\nabla L(m, b)$\n",
    "</center>\n",
    "where $\\vec{\\lambda}_i = (m, b)$, $\\alpha$ is the learning rate and $L(m, b)$ is the mean-squared-loss as a function of the parameters $m$ and $b$.\n",
    "\n",
    "Finding the optimal parameters should get a lot easier. Test the behaviour at different learning rates. Which learning rate seems to be a good choice? Which are the best parameters found like this? How does the loss compare to the one you could find manually?\n",
    "\n",
    "*Note: Due to how the program is animated, every click on 'Descend!' updates the plots two times. Once for the slope, one for the intercept.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8acea56ada940ae877b103a5532294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output())), HBox(children=(VBox(children=(FloatSlider(value=0.0, conti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "linear_example_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use different and better minimization methods later on in the tutorial but this is it for now. We want to look at another type of problem first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Classification works very similar to regression. But instead of assigning a (mostly) continuous value to a combination of inputs, we want to assing a discrete *class* as *prediction*. In the regression problem we were looking for a function that is as similar to the data as possible, for classification we want a function that divides our data into areas or *classes*. In simple cases this can in fact be a straight line line we used before.\n",
    "<img src=\"https://cdn.educba.com/academy/wp-content/uploads/2019/12/Regression-vs-Classification.jpg\" height=200, style=\"height:200px\"> (image: https://www.educba.com/regression-vs-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows you how you can create a *decision boundary* with a linear function. Every value on one side *belongs* to one class, every value on the other side *belongs* to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a64211f345f4a4bac324112c0d58d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), VBox(children=(FloatSlider(value=0.0, continuous_update=False, description='slope m:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "linear_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with regression, deciding the *best* decision boundary is a difficult task. What is a good *loss* for a classification task? Measures like the *accuracy* seems to be a natural measure:\n",
    "<img src=\"https://miro.medium.com/max/4208/1*Yslau43QN1pEU4jkGiq-pw.png\" height=400 style='height: 400px'>\n",
    "(image: https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e4d905ca6144bcbba72735ed64959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), VBox(children=(FloatSlider(value=0.0, continuous_update=False, description='slope m:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "linear_classification_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, that these are indeed good *metrics* to evaluate the performance of a classification. They are not good loss functions though and we have already learned why. First of all, accuracy is not differentiable. It changes in discrete steps depending on whether a value *belongs* to class 1 or class 2. The optimizer won't be happy and we don't want to go back to manual tuning. In addition, \n",
    "\n",
    "Let's leave behind the need to assign each data point to a class and instead assign a *probability* that it belongs to a class. A very commonly used function for this task is the *sigmoid function*.\n",
    "<p>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Sigmoid-function-2.svg\" height=200 style='height: 200px'>\n",
    "</p>\n",
    "\n",
    "The sigmoid function takes the value 0.5 on the decision boundary and rises or falls away from the boundary. So we can assign probabilities to data points. This has not solved the problem of a differentiable loss function. To compute an accuracy, we still need to assign a class label. We would actually prefer to have a loss function that takes the probability values for the data points into account. The most commonly chosen loss function for such *binary classifications* (the class is either 0 or 1 (1 or 2 respectively) is a function called *binary crossentropy* which is often also known as *log loss* and is closely related to *maximum likelihood methods*:\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "    $ b. c. e = - \\frac{1}{N} \\sum_{i = 1}^{N} \\left( y_i\\cdot \\log (p_i) + (1-y_i)\\cdot \\log(1-p_i) \\right)$\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "Here, $p$ is the predicted probability that a given data point belongs to class 1 using the sigmoid function, $y$ is the correct class label. The loss is designed that way, that if the class $y$ is 0, only the part $\\log(1-p)$ counts, if the class $y$ is 1, only $\\log (p)$ counts.\n",
    "\n",
    "<p>\n",
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png\" height=250 style='height:250px'>\n",
    "</p>\n",
    "\n",
    "Let's inspect how the *binary crossentropy* behaves when we move around the decision boundary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31466e54b3414832bab1547722e48295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output())), VBox(children=(FloatSlider(value=0.0, continuous_update=Fa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code!\n",
    "linear_classification_accuracy_and_bce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "At this point you have learned already a lot how a computer is in principle able to predict values based on input data. These were very simple examples but the underlying concepts are really the same for many more applications in machine learning. The next tutorials will investigate how we can approximate more complex functions and use them to tackle regression and classification problems in higher dimensions.\n",
    "At this point you should have learned:\n",
    "\n",
    "- what is supervised learning?\n",
    "- what are the goals of regression and classification?\n",
    "- why do we need a loss function?\n",
    "- what is the difference between a good *metric* and a good *loss*?\n",
    "- what are examples for typical metrics and losses?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
