{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first end-to-end project - the iris data set\n",
    "In the last tutorials you have learned a lot of the basics needed for training neural network models for the approximation of difficult regression or classification functions. In this tutorial we stitch everything together by developing a 'real' model for a more complex task. We will stick with classification for now, a lot can be transferred to regression though by adjusting i. e. the loss function.\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://cdn.educba.com/academy/wp-content/uploads/2019/12/Regression-vs-Classification.jpg\" height=200, style=\"height:200px\"> \n",
    "        (image: https://www.educba.com/regression-vs-classification/)\n",
    "    </center>\n",
    "</p>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris - one of the most famous data sets\n",
    "The *iris* data set is one of the oldest data sets used for testing classification methods. It was developed around 1936 by the British statistician and biologist Ronald Fisher.\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/06/Multi-Class-Classification-Tutorial-with-the-Keras-Deep-Learning-Library.jpg\" height=200, style=\"height:200px\"> \n",
    "        (image: https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/)\n",
    "    </center>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you run this in Colab, you need to download the examples: uncomment the following line\n",
    "# ! git clone https://github.com/flome/e4_bsc_python\n",
    "# % cd e4_bsc_python\n",
    "# ! git checkout machine_learning\n",
    "# % cd 4.\\ Machine\\ Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we will import and inspect the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris_dataset.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://ars.els-cdn.com/content/image/3-s2.0-B9780128147610000034-f03-01-9780128147610.jpg\" height=400, style=\"height:400px\"> \n",
    "        (image: https://www.sciencedirect.com/topics/computer-science/iris-virginica)\n",
    "    </center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *seaborn* to quickly visualize the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing - splitting, data scaling and label encoding\n",
    "We will start as always: preparing the data. As we learned before, we will start by creating a separate test set for later validation from the design matrix and the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the extra brackets! Without these, we don't get a (150, 1) vector but a (150,) 1D vector \n",
    "# that would not comply with machine learning conventions and produces errors along the way\n",
    "Y = data[['species']].values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the target vector? For regression problems with continuous output, we can simply create a Standard Scaler as well. Strings are not very good values as output for e. g. a sigmoid function though.\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Sigmoid-function-2.svg\" height=200 style='height: 200px'>\n",
    "        (image: https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "We will create instead of *one* target variable *three* variables, one representing the probability to belong to one of the three classes. This is called *one-hot encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# the sparse parameter determines whether the full matrix is stored or only the non-zero elements\n",
    "# we stay with the basic matrix version for now\n",
    "target_scaler = OneHotEncoder(sparse=False)\n",
    "Y_train_scaled = target_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = target_scaler.transform(Y_test)\n",
    "print('before encoding: ')\n",
    "print(Y_train[:3])\n",
    "print('after encoding: ')\n",
    "print(Y_train_scaled[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classification model\n",
    "Next, we will create a Keras model for the classification. We will keep it simple to start with. We need 4 input nodes and 3 output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier = Sequential()\n",
    "iris_classifier.add( Input((4,)) )\n",
    "iris_classifier.add( Dense(32, activation='tanh', name='hidden_layer') )\n",
    "iris_classifier.add( Dense(3, activation='sigmoid', name='output_layer') )\n",
    "iris_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "callbacks = iris_classifier.fit(X_train_scaled, Y_train_scaled, validation_split=.15, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(callbacks.history['loss'], label='training loss')\n",
    "plt.plot(callbacks.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('b. c. e. loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that both training and validation loss keep decreasing, the model is not fully trained. The validation loss is higher than the training loss, so we have slight over-training, but nothing to worry to much about yet. Let us try to experiment with a higher learning rate. For this, we need to instantiate an optimizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't create a new model, the weights will continue to be improved from where we left them after the first optimization round above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier = Sequential()\n",
    "iris_classifier.add( Input((4,)) )\n",
    "iris_classifier.add( Dense(32, activation='tanh', name='hidden_layer') )\n",
    "iris_classifier.add( Dense(3, activation='sigmoid', name='output_layer') )\n",
    "iris_classifier.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "callbacks = iris_classifier.fit(X_train_scaled, Y_train_scaled, validation_split=.15, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(callbacks.history['loss'], label='training loss')\n",
    "plt.plot(callbacks.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('b. c. e. loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(callbacks.history['accuracy'], label='training accuracy')\n",
    "plt.plot(callbacks.history['val_accuracy'], label='validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news: the training has converged! The bad news: now we do see worrying over-training... the validation loss starts to increase again and the accuracy is dropping!\n",
    "\n",
    "What now? The simplest approach is to design a simpler model. If we want our model to stay more complex, there are still several ways to reduce over-training by artificially reducing the model capacity during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization\n",
    "Regularization is the category of techniques which constrain the model capacity during the training process. In machine learning, there are two types of regularization which are mainly used. \n",
    "\n",
    "    - regularization by penalizing large weight values\n",
    "    - regularization by dropout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regularization by penalizing large weight values\n",
    "Over-fitting is very often a result of weights in the neural network becoming too extreme. This leads to sharp decision boundaries instead of smooth and regular shapes which better approximate the underlying function we want to approximate. \n",
    "\n",
    "Do you remember the loss function of the binary crossentropy? I am sure you, do but I will put it here again anyway:\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "    $ b. c. e = - \\frac{1}{N} \\sum_{i = 1}^{N} \\left( y_i\\cdot \\log (p_i) + (1-y_i)\\cdot \\log(1-p_i) \\right)$\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "We can make the optimizer take into account the size of the weights by adding an additional bit to this loss. We could for example append an additional *cost* that is proportional to the norm of the weights:\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "    $ {b. c. e}_\\mathrm{regularized} = b. c. e + \\lambda \\cdot \\sqrt{\\sum_{i = 1}^{N} {w_i}^2}$\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "$\\lambda$ is called the *regularization strength* or *regularization parameter*. By doing this, the optimizer cannot minimize the loss anymore by adjusting the weights however he likes because making the weights too extreme simply does not lead to a loss decrease anymore! Because this way of regularizing uses the $l_2$-norm of the weights, it is called $l_2$-regularization. Amongst others, it is available for usage in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_regularizer = regularizers.l2(l=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the regularizer to some layers. Their weights will then be penalized. We don't want to constrain the output, so we add it only to the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris_classifier = Sequential()\n",
    "iris_classifier.add( Input((4,)) )\n",
    "iris_classifier.add( Dense(32, activation='tanh', name='hidden_layer', kernel_regularizer=l2_regularizer) )\n",
    "iris_classifier.add( Dense(3, activation='sigmoid', name='output_layer') )\n",
    "iris_classifier.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "callbacks = iris_classifier.fit(X_train_scaled, Y_train_scaled, validation_split=.15, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(callbacks.history['loss'], label='training loss')\n",
    "plt.plot(callbacks.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('b. c. e. loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! Our overall loss is quite a bit higher than before, but the model generalizes a lot better now! We see this by comparing the losses for the training and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regularization by dropout\n",
    "In the last years, the regularization by weight penalization has received quite some critical opinions because they tend to constrain the model capacity quite aggressively. Sometimes the model simply needs large weight values to do its job. Another method is preferred especially in deep learning often: dropout\n",
    "\n",
    "The concept on dropout is very simple. During the training process, nodes from a layer to which *dropout* is applied are simply left out, their output is set to zero.\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        <img src=\"https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png\n",
    "\" height=300, style=\"height:300px\"> \n",
    "        (image: https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html)\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "Dropout can be added to your Keras model using a *dropout layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier = Sequential()\n",
    "iris_classifier.add( Input((4,)) )\n",
    "iris_classifier.add( Dense(32, activation='tanh', name='hidden_layer' ) )\n",
    "iris_classifier.add( Dropout(rate=0.25) )                    \n",
    "iris_classifier.add( Dense(3, activation='sigmoid', name='output_layer') )\n",
    "iris_classifier.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "callbacks = iris_classifier.fit(X_train_scaled, Y_train_scaled, validation_split=.15, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(callbacks.history['loss'], label='training loss')\n",
    "plt.plot(callbacks.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('b. c. e. loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether regularization is necessary for your model and which one best to choose is, among all the other nice things, part of the *hyperparameter optimization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter optimization - here we go again\n",
    "You can try now to find hyperparamters - a model setup - that fits the given problem best. A little bit of boilerplate is given below, enjoy the ride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt =  Adam(learning_rate=0.01)\n",
    "\n",
    "parameter_grid={\n",
    "    'hidden_nodes': [10, 40, 80],\n",
    "    'hidden_activation': ['tanh'],\n",
    "    'optimizer': [opt]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def get_param_combos(p_grid):\n",
    "    combis = product(*[v for v in p_grid.values()])\n",
    "    return [{key: value for key, value in zip(p_grid.keys(), combo)} for combo in combis] \n",
    "\n",
    "combos = get_param_combos(parameter_grid)\n",
    "print(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parameter_combination(parameters, X, Y):\n",
    "    # create a Sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(4,)))\n",
    "    \n",
    "    # let's keep the number of nodes in the hidden layer and the activation variable\n",
    "    model.add(Dense(parameters['hidden_nodes'], activation=parameters['hidden_activation']))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "   \n",
    "    # we can also vary the optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer=parameters['optimizer'], metrics=['accuracy'])\n",
    "    \n",
    "    # we want to fit the model and return the trained model and the loss history for later inspection\n",
    "    loss_history = model.fit(X, Y, validation_split=.15, epochs=300)\n",
    "    return model, loss_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list that will be filled with the results:\n",
    "losses = []\n",
    "models = []\n",
    "# loop over the parameters\n",
    "# this will take a while!\n",
    "for parameter in combos:\n",
    "    print(\"Testing parameter configuration: {}\".format(parameter))\n",
    "    model, loss = test_parameter_combination(parameter, X_train_scaled, Y_train_scaled)\n",
    "    models.append(model)\n",
    "    \n",
    "    # we will be happy about this weird looking bit just in a second\n",
    "    losses.append({**parameter, 'loss_history': loss})\n",
    "    \n",
    "res = pd.DataFrame(losses)\n",
    "res['final_train_loss'] = res['loss_history'].apply(lambda x: x['loss'][-1])\n",
    "res['final_train_acuracy'] = res['loss_history'].apply(lambda x: x['accuracy'][-1])\n",
    "res['final_val_loss'] = res['loss_history'].apply(lambda x: x['val_loss'][-1])\n",
    "res['final_val_acuracy'] = res['loss_history'].apply(lambda x: x['val_accuracy'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, loss in enumerate(losses):\n",
    "    plt.plot(loss['loss_history']['loss'], label='run {} train'.format(run))\n",
    "    plt.plot(loss['loss_history']['val_loss'], label='run {} val'.format(run))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('b. c. e loss')\n",
    "plt.legend(loc=2, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, loss in enumerate(losses):\n",
    "    plt.plot(loss['loss_history']['accuracy'], label='run {} train'.format(run))\n",
    "    plt.plot(loss['loss_history']['val_accuracy'], label='run {} val'.format(run))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('b. c. e loss')\n",
    "plt.legend(loc=2, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
